{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skbio # scikit-bio belongs to the scipy stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3891d2d",
   "metadata": {},
   "source": [
    "for a library to become part of the scipy ecosystem, it must be based on current scipy libraries.\n",
    "library authors cannot create objects unique to their libraries that are functionally equivalent\n",
    "    to existing scipy or built-in objects.\n",
    "    the rationale is that learning code to get started and be productive is time-intensive.\n",
    "    scipy users should be able to leverage their knowledge of existing scipy objects and built-ins\n",
    "    when using new objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95439dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inList = []\n",
    "# documentation for skbio.io.read() says it's a generator.\n",
    "# python generators can be iterated through only once, after which they're exhausted.\n",
    "# we use the next command to iterate over generators.\n",
    "for seq in skbio.io.read(\"../dataset/All-Unigene1000.fa.bz2\", format='fasta', compression='bz2'):\n",
    "    inList.append(seq) # append record to inList\n",
    "    next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fasta recordset we're using has only 1000 fasta records\n",
    "len(inList) # verifying that inList has as many elements as the file has records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating the scikit-bio objects that we've collected\n",
    "nuclList = inList[0].values # nuclList is an ndarray of byte-chars of the data string\n",
    "nuclList = list(nuclList) # let's coerce the ndarray into a list\n",
    "for index in range(len(nuclList)):\n",
    "    # replace each value in index/value pair\n",
    "    # with its ascii-decoded equivalent\n",
    "    nuclList[index] = nuclList[index].decode('ascii')\n",
    "# split and join are inverse functions\n",
    "# split splits the elements of a string into elements of a list\n",
    "# join joins the elements of a list (when they are all strings)\n",
    "# into a single string\n",
    "nuclStr = ''.join(nuclList) # sep.join(list-of-strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nuclStr) # length of the nucleotide string of the first fasta record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(inList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation on skbio.sequence mentions the metadata, which is collected in a dictionary\n",
    "inList[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612bb14",
   "metadata": {},
   "source": [
    "## now that we have a handle on the scikit-bio object, let's extract the data we want, add to it through computation, then write out our resultset to a datafile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nuclStr(inListPos: int) -> str:\n",
    "    '''Given index of skbio.sequence object in list, return nucleotide string in skbio object.\n",
    "    '''\n",
    "    nuclList = inList[inListPos].values # nuclList is an ndarray of byte-chars\n",
    "    nuclList = list(nuclList) # coerce the ndarray into a list\n",
    "    for index in range(len(nuclList)):\n",
    "        # replace each value in index/value pair\n",
    "        # with its ascii-decoded equivalent\n",
    "        nuclList[index] = nuclList[index].decode('ascii')\n",
    "    return ''.join(nuclList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7fd06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList = []\n",
    "for index in range(len(inList)):\n",
    "    nuclStr = create_nuclStr(index)\n",
    "    recID = inList[index].metadata['id']\n",
    "    record = [recID, nuclStr] # each record is a list\n",
    "    dfList.append(record) # our collection is a list-of-lists, or LoL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ca15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create module, save to working directory\n",
    "import myModule as mM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in dfList:\n",
    "    record.append(mM.get_gc_content_per(record[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList[:5]\n",
    "# what went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's modify the function in our module to allow both upper- and lowercase nucleotide string as input\n",
    "del mM\n",
    "# these magics allow the notebook to reload contents of edited modules\n",
    "# into session memory\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import myModule as mM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de673d30",
   "metadata": {},
   "source": [
    "### now we re-upload our (edited) module, re-run our code, and view the first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f241286",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList = []\n",
    "for index in range(len(inList)):\n",
    "    nuclStr = create_nuclStr(index)\n",
    "    recID = inList[index].metadata['id']\n",
    "    record = [recID, nuclStr] # each record is a list\n",
    "    dfList.append(record) # our collection is a list-of-lists, or LoL\n",
    "    \n",
    "for record in dfList:\n",
    "    record.append(mM.get_gc_content_per(record[1]))\n",
    "    \n",
    "dfList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224364a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a pandas dataframe out of our dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dfList, columns=('UniqID', 'nuclStr', 'gc_content_%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e150162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we want to append string length to each record\n",
    "# it is part of the metadata description string in the scikit-bio objects\n",
    "for index in range(len(inList)):\n",
    "    strLength = inList[index].metadata['description'].split()[1]\n",
    "    dfList[index].append(int(strLength))\n",
    "dfList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remake our pandas dataframe out of our dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dfList, columns=('UniqID', 'nuclStr', 'gc_content_%', 'size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, columns=('UniqID', 'nuclStr', 'size', 'gc_content_%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63061722",
   "metadata": {},
   "source": [
    "# saving our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save our resultset. pandas gives us lots of file output options.\n",
    "# we can save our work to share with colleagues, or to prepare as input for more data processing software.\n",
    "df.to_csv('intermediate_record_set.csv', sep=',')\n",
    "# we can also save our work to disk, with the intention of working on it later, picking up where we left off.\n",
    "df.to_pickle('df.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8335c",
   "metadata": {},
   "source": [
    "### pickling saves our collections to our mass storage drive (hard drive, usb drive, etc) in a format that allows us to restore them directly to python session memory later.\n",
    "### this allows us to work on a collection over time --- update its values, add new values, and delete values --- as we might over long-running projects, writing it out when we stop working on the project, then reading it directly back into memory when we pick up where we left off.\n",
    "### let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94dc51",
   "metadata": {},
   "source": [
    "### in python, the del keyword allows us to delete objects from session memory.\n",
    "### let's delete the df dataframe from memory, then restore it from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos # it's gone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we were starting a new session that we're restoring df to, we'd need to first import pandas ---\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we unpickle df into session memory...\n",
    "df = pd.read_pickle('../pickle/df.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos # it's back! tuh-duh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may want to compress our file...\n",
    "import tarfile\n",
    "with tarfile.open('../dataset/intermediate_record_set.csv.tar.bz2', 'w:bz2') as tar:\n",
    "    tar.add('intermediate_record_set.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
